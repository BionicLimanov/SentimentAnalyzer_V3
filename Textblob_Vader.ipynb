{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYzSyPJW4tLDGLJr5tJEes",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BionicLimanov/SentimentAnalyzer_V3/blob/main/Textblob_Vader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#========================================================= RUN THIS FIRST =========================================================\n",
        "\n",
        "!pip install -U deep-translator\n",
        "!pip install sastrawi\n",
        "!pip install pandas\n",
        "!pip install torch torchvision\n",
        "!pip install vaderSentiment\n",
        "!pip install textblob\n",
        "!pip install article\n",
        "!pip install -U textblob\n",
        "!python -m textblob.download_corpora\n",
        "!pip install requests\n",
        "!pip install newspaper3k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z3JBxo1pc0f",
        "outputId": "15ac0af1-028f-48f9-f38a-fc4a50df0644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.8.3-py3-none-any.whl (29 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.0.4)\n",
            "Installing collected packages: soupsieve, beautifulsoup4, deep-translator\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.11.1 deep-translator-1.8.3 soupsieve-2.3.2.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting article\n",
            "  Downloading article-0.1.1.tar.gz (724 bytes)\n",
            "Building wheels for collected packages: article\n",
            "  Building wheel for article (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for article: filename=article-0.1.1-py3-none-any.whl size=1065 sha256=a13608e963c30e9f2014720c925ed27a4e052a881490aaeb1fa9a7966e8bd012\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/c6/4c/d69c7b9cc5e97ac777e6338421fff7d26fcb7a793c2e3bdeb7\n",
            "Successfully built article\n",
            "Installing collected packages: article\n",
            "Successfully installed article-0.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Installing collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.15.3\n",
            "    Uninstalling textblob-0.15.3:\n",
            "      Successfully uninstalled textblob-0.15.3\n",
            "Successfully installed textblob-0.17.1\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjRp1Z3UDFug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15743bc5-a6ad-467f-f2d7-45fed9f5daaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "bbri anjlok cuy\n",
            "['bbri', 'fell', 'dude']\n",
            "[('bbri', 1), ('fell', 1), ('dude', 1)]\n",
            "\n",
            "\n",
            "\n",
            "NETRAL SIH INI\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "#INI MODEL TEXTBLOB\n",
        "\n",
        "\n",
        "\n",
        "from nltk.tokenize import punkt\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from newspaper import Article\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "import re\n",
        "import urllib\n",
        "from urllib.request import Request, urlopen\n",
        "import json\n",
        "import pandas as pd \n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "lowercasetext = input()\n",
        "translated = GoogleTranslator(source='auto', target='en').translate(lowercasetext) \n",
        "lowercasetext = translated.lower()\n",
        "lowercasetext = lowercasetext.lower()\n",
        "lowercasetext = lowercasetext.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "lowercasetext = re.sub(r\"\\d+\", \"\", lowercasetext)\n",
        "lowercasetext = lowercasetext.strip()\n",
        "tokens = nltk.tokenize.word_tokenize(lowercasetext)\n",
        "custom_stopwords = ['dengan', 'ia', 'bahwa', 'oleh' ',', 'limacorp', 'lia', 'itzy', 'papi', 'elon', 'tele', 'yg']\n",
        "list_saham = 'https://docs.google.com/spreadsheets/d/1SCDTDsI2-UZd0SXLrRS3DVpgxLFvpMdoFeixgFZdjkM/edit?usp=sharing'\n",
        "custom_stopwords.append(list_saham)\n",
        "list_stopwords = set(stopwords.words('indonesian') + custom_stopwords)\n",
        "tokens_dah_bersih = [word for word in tokens if not word in list_stopwords]\n",
        "print(tokens_dah_bersih)\n",
        "\n",
        "#ngitung freq kemunculan kata\n",
        "kemunculan = nltk.FreqDist(tokens_dah_bersih)\n",
        "kemunculan = kemunculan.most_common()\n",
        "print(kemunculan)\n",
        "print('\\n\\n')\n",
        "\n",
        "#buat ngerubah list jdi string spy bisa dicerna sm blob\n",
        "pinal_text = ' '.join(str(e) for e in kemunculan)\n",
        "blob = TextBlob(pinal_text)\n",
        "sentiment = blob.sentiment.polarity # -1 smpe 1\n",
        "if sentiment >= 0.05 :\n",
        "  print(\"this whole thing is positive...NICE\")\n",
        "elif sentiment <= -0.05 : \n",
        "  print(\"WAH ramalan LALA gabagus nih om...NEGATIF\")\n",
        "else : \n",
        "  print(\"NETRAL SIH INI\")\n",
        "print(sentiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INI MODEL VADER\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from deep_translator import GoogleTranslator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('popular')\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "testing_manual = input()\n",
        "translated = GoogleTranslator(source='auto', target='en').translate(testing_manual) \n",
        "lowercasetext = translated.lower()\n",
        "lower = [word.lower() for word in testing_manual]\n",
        "lowercasetext = ' '.join(str(e) for e in lower)\n",
        "lowercasetext = re.sub(r\"\\d+\", \"\", lowercasetext)\n",
        "lowercasetext = re.sub(r\"\\$\", \"\", lowercasetext)\n",
        "lowercasetext = lowercasetext.strip()\n",
        "tokens = nltk.tokenize.word_tokenize(lowercasetext)\n",
        "custom_stopwords = ['dengan', 'ia', 'bahwa', 'oleh' ',', 'lia', 'itzy', 'papi', 'elon', 'tele', 'yg']\n",
        "list_saham = 'https://docs.google.com/spreadsheets/d/1SCDTDsI2-UZd0SXLrRS3DVpgxLFvpMdoFeixgFZdjkM/edit?usp=sharing'\n",
        "custom_stopwords.append(list_saham)\n",
        "list_stopwords = set(stopwords.words('indonesian') + custom_stopwords)\n",
        "tokens_dah_bersih = [word for word in tokens if not word in list_stopwords]\n",
        "tokens_dah_bersih = ' '.join(str(e) for e in tokens_dah_bersih)\n",
        "kemunculan = nltk.FreqDist(tokens_dah_bersih)\n",
        "kemunculan = kemunculan.most_common()\n",
        "#buat ngerubah list jdi string spy bisa dicerna sm blob\n",
        "text = ' '.join(str(e) for e in kemunculan)\n",
        "text = TextBlob(text)\n",
        "blobpolarity = text.sentiment.polarity\n",
        "#mencoba pake main dan \n",
        "def sentimentScores(lowercasetext):\n",
        "    sentiment_object = SentimentIntensityAnalyzer()\n",
        "    hasil_bagi = sentiment_object.polarity_scores(lowercasetext)\n",
        "    print(hasil_bagi)\n",
        "    print(hasil_bagi['neg']*100, \"% Negative\")\n",
        "    print(hasil_bagi['neu']*100, \"% Neutral\")\n",
        "    print(hasil_bagi['pos']*100, \"% Positive\")\n",
        "    if hasil_bagi['compound'] >= 0.05 :\n",
        "        print(\"Positive\")\n",
        "    elif hasil_bagi['compound'] <= - 0.05 :\n",
        "        print(\"Negative\")\n",
        "    else :\n",
        "        print(\"Neutral\")\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "\n",
        "    print('hasil dari vader pre trained data yield')\n",
        "    sentimentScores(lowercasetext)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBgSMN1mobFd",
        "outputId": "a706c49b-d7c9-446c-961a-2cab2eab556d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "bbri anjlok cuy\n",
            "hasil dari vader pre trained data yield\n",
            "{'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.4588}\n",
            "0.0 % Negative\n",
            "80.0 % Neutral\n",
            "20.0 % Positive\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdUsdV6hsXiX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}