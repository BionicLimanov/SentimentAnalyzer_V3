{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcEwkr8VLH56zbpjPYvigp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BionicLimanov/SentimentAnalyzer_V3/blob/main/Textblob_Vader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#========================================================= RUN THIS FIRST =========================================================\n",
        "\n",
        "!pip install -U deep-translator\n",
        "!pip install sastrawi\n",
        "!pip install pandas\n",
        "!pip install torch torchvision\n",
        "!pip install vaderSentiment\n",
        "!pip install textblob\n",
        "!pip install article\n",
        "!pip install -U textblob\n",
        "!python -m textblob.download_corpora\n",
        "!pip install requests\n",
        "!pip install newspaper3k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z3JBxo1pc0f",
        "outputId": "a8361667-8208-4de6-fb8b-534c8bcd0dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.8.3-py3-none-any.whl (29 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Installing collected packages: soupsieve, beautifulsoup4, deep-translator\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.11.1 deep-translator-1.8.3 soupsieve-2.3.2.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 3.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting article\n",
            "  Downloading article-0.1.1.tar.gz (724 bytes)\n",
            "Building wheels for collected packages: article\n",
            "  Building wheel for article (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for article: filename=article-0.1.1-py3-none-any.whl size=1065 sha256=09b90e5c69b41a1c1eb658ecf73c0402ce4e199606383cda12364c0787a6e3cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/c6/4c/d69c7b9cc5e97ac777e6338421fff7d26fcb7a793c2e3bdeb7\n",
            "Successfully built article\n",
            "Installing collected packages: article\n",
            "Successfully installed article-0.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Installing collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.15.3\n",
            "    Uninstalling textblob-0.15.3:\n",
            "      Successfully uninstalled textblob-0.15.3\n",
            "Successfully installed textblob-0.17.1\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.11.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.7)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 29.3 MB/s \n",
            "\u001b[?25hCollecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "Collecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.3.1-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.6.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2022.6.15)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.8.0)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=df651bb4783d14d61a72380ece01dde34b57bddf209c942dcf0bf57d3360dd90\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=855dc1c8e20164acf189ac924cf349d1c56f4ca55c83dc5eaf9d10758d0ec883\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=5bb8d3b515acae8092ee776826084d9bdcae6e056b002add65808ba3245e71b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=3e349c341100ba221fc0784b39e831ad45fa00531e2102035576d9f5319b9a91\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjRp1Z3UDFug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fb858e-69b8-4e9e-86a8-23d58b1b7767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "tlkm membunuh\n",
            "['tlkm', 'kill']\n",
            "[('tlkm', 1), ('kill', 1)]\n",
            "\n",
            "\n",
            "\n",
            "('tlkm', 1) ('kill', 1)\n",
            "NETRAL SIH INI\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "#INI MODEL TEXTBLOB\n",
        "\n",
        "\n",
        "\n",
        "from nltk.tokenize import punkt\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from newspaper import Article\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "import re\n",
        "import urllib\n",
        "from urllib.request import Request, urlopen\n",
        "import json\n",
        "import pandas as pd \n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "lowercasetext = input()\n",
        "translated = GoogleTranslator(source='auto', target='en').translate(lowercasetext) \n",
        "lowercasetext = translated.lower()\n",
        "lowercasetext = lowercasetext.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "lowercasetext = re.sub(r\"\\d+\", \"\", lowercasetext)\n",
        "lowercasetext = lowercasetext.strip()\n",
        "tokens = nltk.tokenize.word_tokenize(lowercasetext)\n",
        "custom_stopwords = ['dengan', 'ia', 'bahwa', 'oleh' ',', 'limacorp', 'lia', 'itzy', 'papi', 'elon', 'tele', 'yg']\n",
        "list_saham = 'https://docs.google.com/spreadsheets/d/1SCDTDsI2-UZd0SXLrRS3DVpgxLFvpMdoFeixgFZdjkM/edit?usp=sharing'\n",
        "custom_stopwords.append(list_saham)\n",
        "list_stopwords = set(stopwords.words('indonesian') + custom_stopwords)\n",
        "tokens_dah_bersih = [word for word in tokens if not word in list_stopwords]\n",
        "print(tokens_dah_bersih)\n",
        "\n",
        "#ngitung freq kemunculan kata\n",
        "kemunculan = nltk.FreqDist(tokens_dah_bersih)\n",
        "kemunculan = kemunculan.most_common()\n",
        "print(kemunculan)\n",
        "print('\\n\\n')\n",
        "\n",
        "#buat ngerubah list jdi string spy bisa dicerna sm blob\n",
        "pinal_text = ' '.join(str(e) for e in kemunculan)\n",
        "print(pinal_text)\n",
        "blob = TextBlob(lowercasetext)\n",
        "sentiment = blob.sentiment.polarity # -1 smpe 1\n",
        "if sentiment >= 0.05 :\n",
        "  print(\"this whole thing is positive...NICE\")\n",
        "elif sentiment <= -0.05 : \n",
        "  print(\"WAH ramalan LALA gabagus nih om...NEGATIF\")\n",
        "else : \n",
        "  print(\"NETRAL SIH INI\")\n",
        "print(sentiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INI MODEL VADER\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from deep_translator import GoogleTranslator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('popular')\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "testing_manual = input()\n",
        "translated = GoogleTranslator(source='auto', target='en').translate(testing_manual) \n",
        "lowercasetext = translated.lower()\n",
        "lower = [word.lower() for word in testing_manual]\n",
        "lowercasetext = ' '.join(str(e) for e in lower)\n",
        "lowercasetext = re.sub(r\"\\d+\", \"\", lowercasetext)\n",
        "lowercasetext = re.sub(r\"\\$\", \"\", lowercasetext)\n",
        "lowercasetext = lowercasetext.strip()\n",
        "tokens = nltk.tokenize.word_tokenize(lowercasetext)\n",
        "custom_stopwords = ['dengan', 'ia', 'bahwa', 'oleh' ',', 'lia', 'itzy', 'papi', 'elon', 'tele', 'yg']\n",
        "list_saham = 'https://docs.google.com/spreadsheets/d/1SCDTDsI2-UZd0SXLrRS3DVpgxLFvpMdoFeixgFZdjkM/edit?usp=sharing'\n",
        "custom_stopwords.append(list_saham)\n",
        "list_stopwords = set(stopwords.words('indonesian') + custom_stopwords)\n",
        "tokens_dah_bersih = [word for word in tokens if not word in list_stopwords]\n",
        "tokens_dah_bersih = ' '.join(str(e) for e in tokens_dah_bersih)\n",
        "kemunculan = nltk.FreqDist(tokens_dah_bersih)\n",
        "kemunculan = kemunculan.most_common()\n",
        "#buat ngerubah list jdi string spy bisa dicerna sm blob\n",
        "text = ' '.join(str(e) for e in kemunculan)\n",
        "text = TextBlob(text)\n",
        "blobpolarity = text.sentiment.polarity\n",
        "#mencoba pake main dan \n",
        "def sentimentScores(lowercasetext):\n",
        "    sentiment_object = SentimentIntensityAnalyzer()\n",
        "    hasil_bagi = sentiment_object.polarity_scores(lowercasetext)\n",
        "    print(hasil_bagi)\n",
        "    print(hasil_bagi['neg']*100, \"% Negative\")\n",
        "    print(hasil_bagi['neu']*100, \"% Neutral\")\n",
        "    print(hasil_bagi['pos']*100, \"% Positive\")\n",
        "    if hasil_bagi['compound'] >= 0.05 :\n",
        "        print(\"Positive\")\n",
        "    elif hasil_bagi['compound'] <= - 0.05 :\n",
        "        print(\"Negative\")\n",
        "    else :\n",
        "        print(\"Neutral\")\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "\n",
        "    print('hasil dari vader pre trained data yield')\n",
        "    sentimentScores(lowercasetext)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBgSMN1mobFd",
        "outputId": "a706c49b-d7c9-446c-961a-2cab2eab556d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "bbri anjlok cuy\n",
            "hasil dari vader pre trained data yield\n",
            "{'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.4588}\n",
            "0.0 % Negative\n",
            "80.0 % Neutral\n",
            "20.0 % Positive\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import output\n",
        "output.clear()\n",
        "\n",
        "\n",
        "testing_manual = pd.read_excel('./drive/MyDrive/data_ori/CS-3k/LABELLED_BBRI_MANUAL.xlsx')\n",
        "print(testing_manual)\n",
        "# os.system('cls')\n",
        "output.clear()\n",
        "print(testing_manual)\n",
        "print(\"ini data yang di load\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"ini data yang di-predict\")\n",
        "actual = testing_manual[\"Review\"]\n",
        "print(actual)\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "gdUsdV6hsXiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1975ea-219a-49fb-828c-8a4ad3d2ddfa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Review  polar\n",
            "0     ada 5 saham yg sudah di analisis secara teknik...    0.0\n",
            "1      akan kembali ke harga fundamentalnya  At leas...    1.0\n",
            "2     \\nSistem stokbit atau Securitas Securitas sang...    0.0\n",
            "3          \\ndownload Bank Jago, buka rekening pake ...    0.0\n",
            "4     Mari gabung KOMUNITAS SAHAM dengan pembahasan ...    0.0\n",
            "...                                                 ...    ...\n",
            "3168  Jakarta - Pendaftaran programÂ Rekrutmen Bersa...    0.0\n",
            "3169  Salah satu pos yang mencolok di laporan keuang...    0.0\n",
            "3170  Review Saham Part 490 telah tersedia, ada saha...    0.0\n",
            "3171  2 saham Perdagangan Ritel , Pendapatan &amp; L...    0.0\n",
            "3172  Uang THR beli saham apa?\\n\\nUntuk coba tetep k...    1.0\n",
            "\n",
            "[3173 rows x 2 columns]\n",
            "ini data yang di load\n",
            "\n",
            "\n",
            "\n",
            "ini data yang di-predict\n",
            "0       ada 5 saham yg sudah di analisis secara teknik...\n",
            "1        akan kembali ke harga fundamentalnya  At leas...\n",
            "2       \\nSistem stokbit atau Securitas Securitas sang...\n",
            "3            \\ndownload Bank Jago, buka rekening pake ...\n",
            "4       Mari gabung KOMUNITAS SAHAM dengan pembahasan ...\n",
            "                              ...                        \n",
            "3168    Jakarta - Pendaftaran programÂ Rekrutmen Bersa...\n",
            "3169    Salah satu pos yang mencolok di laporan keuang...\n",
            "3170    Review Saham Part 490 telah tersedia, ada saha...\n",
            "3171    2 saham Perdagangan Ritel , Pendapatan &amp; L...\n",
            "3172    Uang THR beli saham apa?\\n\\nUntuk coba tetep k...\n",
            "Name: Review, Length: 3173, dtype: object\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mencoba pake main dan \n",
        "def sentimentScores(tokens_dah_bersih):\n",
        "    sentiment_object = SentimentIntensityAnalyzer()\n",
        "    hasil_bagi = sentiment_object.polarity_scores(tokens_dah_bersih)\n",
        "    print(hasil_bagi)\n",
        "    print(hasil_bagi['neg']*100, \"% Negative\")\n",
        "    print(hasil_bagi['neu']*100, \"% Neutral\")\n",
        "    print(hasil_bagi['pos']*100, \"% Positive\")\n",
        "    if hasil_bagi['neg'] >= hasil_bagi['pos'] and hasil_bagi['neg'] >= hasil_bagi['neu'] :\n",
        "      print(\"negative\")\n",
        "    if hasil_bagi['pos'] >= hasil_bagi['neu'] and hasil_bagi['pos'] >= hasil_bagi['neg'] :\n",
        "      print(\"positive\")\n",
        "    else : \n",
        "      print(\"neutral\")\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "\n",
        "    print('\\n\\nhasil dari vader pre trained data yield')\n",
        "    sentimentScores(tokens_dah_bersih)"
      ],
      "metadata": {
        "id": "Dl7ugS4f04oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODCanC1cRKcF",
        "outputId": "ac1b3153-8cbd-4b12-8cc3-71cd12e99a51"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ tinggal di run ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "\n",
        "# !pip install -U deep-translator\n",
        "# !pip install sastrawi\n",
        "# !pip install pandas\n",
        "# !pip install torch torchvision\n",
        "# !pip install vaderSentiment\n",
        "# !pip install textblob\n",
        "# !pip install article\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from deep_translator import GoogleTranslator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('popular')\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "\n",
        "train = 'https://raw.githubusercontent.com/BionicLimanov/textblob_train/main/blob_train.csv'\n",
        "test = 'https://raw.githubusercontent.com/BionicLimanov/labelbbca/main/4k_bbca_comment%20-%20merge-csv.com__62f491239130a.csv'\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "%ls ./drive/MyDrive/data_ori/CS-3k/LABELLED_BBRI_MANUAL.xlsx\n",
        "testing_manual = pd.read_excel('./drive/MyDrive/data_ori/CS-3k/LABELLED_BBRI_MANUAL.xlsx') \n",
        "# testing_manual = pd.read_excel('/content/drive/MyDrive/data_ori/CS-3k/dataset_saham_icimtech.xlsx')\n",
        "print(\"\\n\\n\")\n",
        "print(\"ini data yang di-predict\")\n",
        "actual = testing_manual[\"Review\"]\n",
        "# actual = testing_manual[\"content_original\"]\n",
        "print(actual)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "lowercasetext = ' '.join(str(e) for e in actual)\n",
        "lowercasetext = re.sub(r\"\\d+\", \"\", lowercasetext)\n",
        "lowercasetext = re.sub(r\"\\$\", \"\", lowercasetext)\n",
        "lowercasetext = lowercasetext.strip()\n",
        "tokens = nltk.tokenize.word_tokenize(lowercasetext)\n",
        "custom_stopwords = ['dengan', 'ia', 'bahwa', 'oleh' ',', 'lia', 'itzy', 'papi', 'elon', 'tele', 'yg']\n",
        "list_saham = 'https://docs.google.com/spreadsheets/d/1SCDTDsI2-UZd0SXLrRS3DVpgxLFvpMdoFeixgFZdjkM/edit?usp=sharing'\n",
        "custom_stopwords.append(list_saham)\n",
        "list_stopwords = set(stopwords.words('indonesian') + custom_stopwords)\n",
        "tokens_dah_bersih = [word for word in tokens if not word in list_stopwords]\n",
        "tokens_dah_bersih = ' '.join(str(e) for e in tokens_dah_bersih)\n",
        "tokens_dah_bersih = TextBlob(tokens_dah_bersih)\n",
        "\n",
        "# 15xloop starts here\n",
        "for x in range(15):\n",
        "  blobpolarity = tokens_dah_bersih.sentiment.polarity\n",
        "  blobpolarity2 = tokens_dah_bersih.sentiment\n",
        "  print(x+1,\".\",'pre trained textblob with deep-translator and sastrawi stop_word yield')\n",
        "  if blobpolarity >= 0.05 :\n",
        "    print(blobpolarity, \"POSITIVE\\n\")\n",
        "  elif blobpolarity <= -0.05 : \n",
        "    print(blobpolarity, \"NEGATIF\\n\")\n",
        "  else : \n",
        "    print(blobpolarity, \"NETRAL\\n\")\n",
        "  \n",
        "\n",
        "#15x loop vader\n",
        "for y in range(15):\n",
        "  sentiment_object = SentimentIntensityAnalyzer()\n",
        "  hasil_bagi = sentiment_object.polarity_scores(tokens_dah_bersih)\n",
        "  print(hasil_bagi)\n",
        "  print(\"hasil dari vader pre trained data yield\")\n",
        "  if hasil_bagi['neg'] >= hasil_bagi['pos'] and hasil_bagi['neg'] >= hasil_bagi['neu'] :\n",
        "    print(y+1,\".\"\"negative\\n\")\n",
        "  if hasil_bagi['pos'] >= hasil_bagi['neu'] and hasil_bagi['pos'] >= hasil_bagi['neg'] :\n",
        "    print(y+1,\".\"\"positive\\n\")\n",
        "  else : \n",
        "    print(y+1,\".\"\"neutral\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AymHI72OpGaR",
        "outputId": "dc673d9b-b335-400c-eda2-8b370cdcd1e4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "./drive/MyDrive/data_ori/CS-3k/LABELLED_BBRI_MANUAL.xlsx\n",
            "\n",
            "\n",
            "\n",
            "ini data yang di-predict\n",
            "0       ada 5 saham yg sudah di analisis secara teknik...\n",
            "1        akan kembali ke harga fundamentalnya  At leas...\n",
            "2       \\nSistem stokbit atau Securitas Securitas sang...\n",
            "3            \\ndownload Bank Jago, buka rekening pake ...\n",
            "4       Mari gabung KOMUNITAS SAHAM dengan pembahasan ...\n",
            "                              ...                        \n",
            "3168    Jakarta - Pendaftaran programÂ Rekrutmen Bersa...\n",
            "3169    Salah satu pos yang mencolok di laporan keuang...\n",
            "3170    Review Saham Part 490 telah tersedia, ada saha...\n",
            "3171    2 saham Perdagangan Ritel , Pendapatan &amp; L...\n",
            "3172    Uang THR beli saham apa?\\n\\nUntuk coba tetep k...\n",
            "Name: Review, Length: 3173, dtype: object\n",
            "\n",
            "\n",
            "\n",
            "1 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "2 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "3 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "4 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "5 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "6 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "7 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "8 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "9 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "10 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "11 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "12 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "13 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "14 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "15 . pre trained textblob with deep-translator and sastrawi stop_word yield\n",
            "0.05673346810623556 POSITIVE\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "1 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "2 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "3 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "4 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "5 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "6 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "7 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "8 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "9 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "10 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "11 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "12 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "13 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "14 .neutral\n",
            "\n",
            "{'neg': 0.023, 'neu': 0.93, 'pos': 0.047, 'compound': 1.0}\n",
            "hasil dari vader pre trained data yield\n",
            "15 .neutral\n",
            "\n"
          ]
        }
      ]
    }
  ]
}